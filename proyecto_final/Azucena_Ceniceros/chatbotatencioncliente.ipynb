{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHATBOT PARA SERVICIO AL CLIENTE\n",
        "\n",
        "A continuación se muestra el código para un chatbot para servicio al cliente. Su objetivo es tomar los terminos y condiciones de una aerolinea y en base a ellos contestar preguntas especificas referentes a esta información."
      ],
      "metadata": {
        "id": "IMtDgafd4PGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hUwkJbx1mx9"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers transformers python-docx scikit-learn numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "re_ranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
      ],
      "metadata": {
        "id": "C8G1xkDQ10x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re"
      ],
      "metadata": {
        "id": "SmAhnAHQ1409"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leer_docx_estructurado(path):\n",
        "    doc = Document(path)\n",
        "    parrafos = []\n",
        "\n",
        "    for p in doc.paragraphs:\n",
        "        estilo = p.style.name if p.style else \"\"\n",
        "        texto = p.text.strip()\n",
        "        if not texto:\n",
        "            continue\n",
        "        parrafos.append({\"texto\": texto, \"estilo\": estilo})\n",
        "    return parrafos\n",
        "\n",
        "\n",
        "def _es_lista(texto):\n",
        "    return bool(re.match(r\"^\\s*[-•*]|\\d+\\.\", texto.strip()))"
      ],
      "metadata": {
        "id": "nHxpC5g22WCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_chunks_por_titulo(parrafos):\n",
        "    chunks = []\n",
        "    actual_titulo = \"\"\n",
        "    contenido_actual = []\n",
        "\n",
        "    for p in parrafos:\n",
        "        texto = p[\"texto\"]\n",
        "        estilo = p[\"estilo\"]\n",
        "        es_titulo = estilo.startswith(\"Heading\") or re.match(r\"^\\d+(\\.\\d+)*\\s\", texto)\n",
        "\n",
        "        if es_titulo:\n",
        "            if actual_titulo and contenido_actual:\n",
        "                chunk = f\"{actual_titulo}\\n\" + \"\\n\".join(contenido_actual)\n",
        "                if len(chunk) > 40:\n",
        "                    chunks.append(chunk)\n",
        "            actual_titulo = texto\n",
        "            contenido_actual = []\n",
        "        else:\n",
        "            contenido_actual.append(texto)\n",
        "\n",
        "    # last chunk\n",
        "    if actual_titulo and contenido_actual:\n",
        "        chunk = f\"{actual_titulo}\\n\" + \"\\n\".join(contenido_actual)\n",
        "        if len(chunk) > 40:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "X0MRKX3k2cYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guardar_chunks_en_txt(chunks, ruta_salida=\"chunks_generados.txt\"):\n",
        "    with open(ruta_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            f.write(f\"--- Chunk {i + 1} ---\\n{chunk}\\n\\n\")\n",
        "    print(f\"Chunks guardados\")"
      ],
      "metadata": {
        "id": "3QsLwyGl2nrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_chunks_relevantes(pregunta, chunks, embeddings_chunks, top_k=5, top_n_final=2):\n",
        "    embedding_pregunta = modelo_embeddings.encode([pregunta], convert_to_tensor=True)\n",
        "    pregunta_np = embedding_pregunta.cpu().numpy()\n",
        "    chunks_np = embeddings_chunks.cpu().numpy()\n",
        "    similitudes = cosine_similarity(pregunta_np, chunks_np)[0]\n",
        "\n",
        "    indices_top_k = np.argsort(similitudes)[-top_k:]\n",
        "    candidatos = [chunks[i] for i in indices_top_k]\n",
        "    pares = [(pregunta, chunk) for chunk in candidatos]\n",
        "    scores = re_ranker.predict(pares)\n",
        "\n",
        "    indices_ordenados = np.argsort(scores)[::-1]  # Mayor a menor\n",
        "    chunks_seleccionados = [candidatos[i] for i in indices_ordenados[:top_n_final]]\n",
        "\n",
        "    return chunks_seleccionados, scores[indices_ordenados[0]]\n",
        "\n",
        "\n",
        "\n",
        "def generar_respuesta(prompt, max_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=max_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_k=1,\n",
        "        top_p=0.8\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "vbexz9sK2rag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_con_busqueda_semantica(pregunta, chunks, embeddings_chunks):\n",
        "    contextos, score = buscar_chunks_relevantes(pregunta, chunks, embeddings_chunks, top_k=5, top_n_final=2)\n",
        "    contexto = \"\\n\\n\".join(contextos)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Based ONLY on the documentation below, answer the question completely.\n",
        "\n",
        "If there are multiple situations or scenarios in the documentation, then list and explain each scenario separately.\n",
        "\n",
        "Use bullet points if needed and DO NOT add information that is not present in the documentation.\n",
        "\n",
        "Documentation:\n",
        "\\\"\\\"\\\"{contexto}\\\"\\\"\\\"\n",
        "\n",
        "Question: {pregunta}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    output = generar_respuesta(prompt)\n",
        "    end = time.time()\n",
        "    return output.split(\"Answer:\")[-1].strip()"
      ],
      "metadata": {
        "id": "IzqwAQxB27ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "DNcNyzo63BZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_docx = \"/content/chatbottext.docx\"\n",
        "print(\"Procesando documento...\")\n",
        "parrafos = leer_docx_estructurado(ruta_docx)\n",
        "chunks = extraer_chunks_por_titulo(parrafos)\n",
        "\n",
        "# Guardar los chunks generados\n",
        "guardar_chunks_en_txt(chunks)\n",
        "\n",
        "print(f\"Calculando embeddings\")\n",
        "embeddings_chunks = modelo_embeddings.encode(chunks, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "shDLehy91tuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo uso del chatbot"
      ],
      "metadata": {
        "id": "rWv_aaTk3clo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"What documents are needed to check in?\"\n",
        "respuesta = responder_con_busqueda_semantica(pregunta, chunks, embeddings_chunks)\n",
        "\n",
        "print(\"Respuesta del chatbot:\")\n",
        "print(respuesta)\n"
      ],
      "metadata": {
        "id": "j8yYb_pV3UMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INTERFAZ GRAFICA"
      ],
      "metadata": {
        "id": "sPCstkpq3qrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "oAlcy14N3mAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def responder_interfaz(pregunta_usuario):\n",
        "    respuesta = responder_con_busqueda_semantica(pregunta_usuario, chunks, embeddings_chunks)\n",
        "    return respuesta\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=responder_interfaz,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Haz tu pregunta aqui...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Chatbot\",\n",
        "    description=\"Este chatbot responde en base a los terminos y condiciones de la aerolínea\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "75Hy64Ok3oy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}