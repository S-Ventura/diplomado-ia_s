{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet Finder System with Image Similarity\n",
    "\n",
    "This notebook demonstrates a proof-of-concept for a pet finder system that uses image similarity to identify potential matches for lost pets. The system extracts features from pet images focusing on shape and fur/hair characteristics and implements similarity search to find the most similar pets in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, MobileNetV2, EfficientNetB3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from IPython.display import Image, display\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.color import rgb2hsv\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "For this demonstration, we'll use the Oxford-IIIT Pet Dataset, which contains images of 37 pet breeds with roughly 200 images for each class. This dataset is perfect for our pet finder system as it contains various breeds with different fur patterns and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download Oxford Pets dataset\n",
    "def download_oxford_pets():\n",
    "    # URLs for the dataset\n",
    "    images_url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
    "    annotations_url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
    "    \n",
    "    # Download and extract images\n",
    "    print(\"Downloading and extracting images...\")\n",
    "    images_path = os.path.join(data_dir, \"images.tar.gz\")\n",
    "    if not os.path.exists(images_path):\n",
    "        urllib.request.urlretrieve(images_url, images_path)\n",
    "        os.system(f\"tar -xzf {images_path} -C {data_dir}\")\n",
    "    \n",
    "    # Download and extract annotations\n",
    "    print(\"Downloading and extracting annotations...\")\n",
    "    annotations_path = os.path.join(data_dir, \"annotations.tar.gz\")\n",
    "    if not os.path.exists(annotations_path):\n",
    "        urllib.request.urlretrieve(annotations_url, annotations_path)\n",
    "        os.system(f\"tar -xzf {annotations_path} -C {data_dir}\")\n",
    "    \n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "\n",
    "# Uncomment the line below to download the dataset\n",
    "# download_oxford_pets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can use the built-in tensorflow datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset\n",
    "def load_oxford_pets_dataset():\n",
    "    print(\"Loading Oxford-IIIT Pet Dataset...\")\n",
    "    dataset, info = tfds.load('oxford_iiit_pet', with_info=True, as_supervised=True)\n",
    "    return dataset, info\n",
    "\n",
    "# Uncomment the line below to load the dataset using tensorflow_datasets\n",
    "# dataset, info = load_oxford_pets_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extractor\n",
    "\n",
    "Now, let's implement the feature extractor using a pre-trained model. We'll use ResNet50 as our base model, but we'll also implement options for VGG16 and MobileNetV2 to experiment with different feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetFeatureExtractor:\n",
    "    def __init__(self, model_type=\"resnet50\"):\n",
    "        self.model_type = model_type\n",
    "        self.input_shape = (224, 224, 3)\n",
    "        self.model, self.preprocess_func = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build a pre-trained model for feature extraction\"\"\"\n",
    "        if self.model_type == \"resnet50\":\n",
    "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=self.input_shape, pooling='avg')\n",
    "            preprocess = resnet_preprocess\n",
    "        elif self.model_type == \"vgg16\":\n",
    "            base_model = VGG16(weights='imagenet', include_top=False, input_shape=self.input_shape, pooling='avg')\n",
    "            preprocess = vgg_preprocess\n",
    "        elif self.model_type == \"mobilenetv2\":\n",
    "            base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=self.input_shape, pooling='avg')\n",
    "            preprocess = mobilenet_preprocess\n",
    "        elif self.model_type == \"efficientnet\":\n",
    "            base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=self.input_shape, pooling='avg')\n",
    "            preprocess = efficientnet_preprocess\n",
    "        else:\n",
    "            raise ValueError(f\"Model type {self.model_type} not supported\")\n",
    "            \n",
    "        return base_model, preprocess\n",
    "    \n",
    "    def extract_features(self, img_path):\n",
    "        \"\"\"Extract features from a single image\"\"\"\n",
    "        # Load and preprocess the image\n",
    "        img = image.load_img(img_path, target_size=(self.input_shape[0], self.input_shape[1]))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = self.preprocess_func(img_array)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.model.predict(img_array, verbose=0)\n",
    "        return features[0]  # Return the feature vector\n",
    "    \n",
    "    def extract_features_batch(self, img_paths, batch_size=32):\n",
    "        \"\"\"Extract features from a batch of images\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Process images in batches to avoid memory issues\n",
    "        for i in tqdm(range(0, len(img_paths), batch_size)):\n",
    "            batch_paths = img_paths[i:i+batch_size]\n",
    "            batch_size = len(batch_paths)\n",
    "            img_arrays = np.zeros((batch_size, self.input_shape[0], self.input_shape[1], 3))\n",
    "            \n",
    "            for j, img_path in enumerate(batch_paths):\n",
    "                try:\n",
    "                    img = image.load_img(img_path, target_size=(self.input_shape[0], self.input_shape[1]))\n",
    "                    img_arrays[j] = image.img_to_array(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    # Use zeros for failed images\n",
    "                    img_arrays[j] = np.zeros(self.input_shape)\n",
    "            \n",
    "            img_arrays = self.preprocess_func(img_arrays)\n",
    "            batch_features = self.model.predict(img_arrays, verbose=0)\n",
    "            features.append(batch_features)\n",
    "        \n",
    "        return np.vstack(features)\n",
    "    \n",
    "    def extract_features_from_array(self, img_array):\n",
    "        \"\"\"Extract features from a preprocessed image array\"\"\"\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = self.preprocess_func(img_array)\n",
    "        features = self.model.predict(img_array, verbose=0)\n",
    "        return features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pet Database\n",
    "\n",
    "Next, let's implement a simple in-memory database to store pet information and features. In a real-world application, this would be a full database, but for our PoC, we'll use pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDatabase:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pet database\"\"\"\n",
    "        # Create a DataFrame to store pet metadata\n",
    "        self.pets_df = pd.DataFrame(columns=[\n",
    "            'id', 'name', 'species', 'breed', 'color', \n",
    "            'location', 'date_added', 'status', 'image_path'\n",
    "        ])\n",
    "        \n",
    "        # Store features separately as they're large\n",
    "        self.features = []\n",
    "        self.next_id = 1\n",
    "    \n",
    "    def add_pet(self, metadata, features):\n",
    "        \"\"\"Add a pet to the database with its metadata and features\"\"\"\n",
    "        # Assign an ID and add to metadata\n",
    "        pet_id = self.next_id\n",
    "        self.next_id += 1\n",
    "        \n",
    "        # Add current date if not provided\n",
    "        if 'date_added' not in metadata:\n",
    "            metadata['date_added'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Add to DataFrame\n",
    "        pet_data = {'id': pet_id, **metadata}\n",
    "        self.pets_df = pd.concat([self.pets_df, pd.DataFrame([pet_data])], ignore_index=True)\n",
    "        \n",
    "        # Store features\n",
    "        self.features.append(features)\n",
    "        \n",
    "        return pet_id\n",
    "    \n",
    "    def search_similar_pets(self, query_features, top_n=5, status=None, location=None, max_days=None):\n",
    "        \"\"\"Find pets with similar features to the query\"\"\"\n",
    "        if len(self.features) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Convert features to numpy array\n",
    "        features_array = np.array(self.features)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity([query_features], features_array)[0]\n",
    "        \n",
    "        # Create a copy of the DataFrame to filter\n",
    "        filtered_df = self.pets_df.copy()\n",
    "        \n",
    "        # Apply filters\n",
    "        if status:\n",
    "            filtered_df = filtered_df[filtered_df['status'] == status]\n",
    "        \n",
    "        if location:\n",
    "            filtered_df = filtered_df[filtered_df['location'] == location]\n",
    "        \n",
    "        if max_days:\n",
    "            # Calculate days since added\n",
    "            today = datetime.now().date()\n",
    "            filtered_df['days_since'] = filtered_df['date_added'].apply(\n",
    "                lambda x: (today - datetime.strptime(x, '%Y-%m-%d').date()).days\n",
    "            )\n",
    "            filtered_df = filtered_df[filtered_df['days_since'] <= max_days]\n",
    "        \n",
    "        # Get indices that remain after filtering\n",
    "        valid_indices = filtered_df.index.tolist()\n",
    "        \n",
    "        # Add similarity to the DataFrame only for valid indices\n",
    "        results_df = filtered_df.copy()\n",
    "        results_df['similarity'] = np.nan\n",
    "        \n",
    "        for i in valid_indices:\n",
    "            if i < len(similarities):\n",
    "                results_df.loc[i, 'similarity'] = similarities[i]\n",
    "        \n",
    "        # Sort by similarity and get top N\n",
    "        results_df = results_df.dropna(subset=['similarity'])\n",
    "        results_df = results_df.sort_values('similarity', ascending=False).head(top_n)\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        results = results_df.to_dict('records')\n",
    "        return results\n",
    "    \n",
    "    def get_pet_by_id(self, pet_id):\n",
    "        \"\"\"Retrieve a pet by its ID\"\"\"\n",
    "        pet = self.pets_df[self.pets_df['id'] == pet_id]\n",
    "        if len(pet) == 0:\n",
    "            return None\n",
    "        return pet.iloc[0].to_dict()\n",
    "    \n",
    "    def update_pet_status(self, pet_id, new_status):\n",
    "        \"\"\"Update the status of a pet (lost, found, reunited)\"\"\"\n",
    "        idx = self.pets_df[self.pets_df['id'] == pet_id].index\n",
    "        if len(idx) > 0:\n",
    "            self.pets_df.loc[idx[0], 'status'] = new_status\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_features_array(self):\n",
    "        \"\"\"Get all features as a numpy array\"\"\"\n",
    "        return np.array(self.features)\n",
    "    \n",
    "    def get_all_pets(self):\n",
    "        \"\"\"Get all pets in the database\"\"\"\n",
    "        return self.pets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demo with Sample Data\n",
    "\n",
    "For demonstration purposes, we'll create a small dataset from the Oxford-IIIT Pet Dataset. We'll extract features from a subset of images and simulate a lost pet scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the images directory exists, otherwise use a small demo dataset\n",
    "def get_image_paths(directory=None, limit=100):\n",
    "    \"\"\"Get image paths from directory or use demo images\"\"\"\n",
    "    if directory and os.path.exists(directory):\n",
    "        image_paths = []\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "        \n",
    "        # Limit the number of images for demo\n",
    "        if limit and len(image_paths) > limit:\n",
    "            return random.sample(image_paths, limit)\n",
    "        return image_paths\n",
    "    else:\n",
    "        # Use demo images or download a small sample\n",
    "        print(\"Using demo images...\")\n",
    "        demo_dir = os.path.join(data_dir, \"demo_images\")\n",
    "        os.makedirs(demo_dir, exist_ok=True)\n",
    "        \n",
    "        # Add code here to download some sample images\n",
    "        # For simplicity, we'll just return an empty list for now\n",
    "        return []\n",
    "\n",
    "# Function to extract breed from filename\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"Extract breed and species from Oxford-IIIT Pet Dataset filename\"\"\"\n",
    "    # Example: Abyssinian_1.jpg\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    breed = parts[0].replace('_', ' ')\n",
    "    \n",
    "    # Determine species (cat or dog)\n",
    "    cat_breeds = ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British', 'Egyptian', 'Maine', 'Persian', 'Ragdoll', 'Russian', 'Siamese', 'Sphynx']\n",
    "    species = 'cat' if any(cat in breed for cat in cat_breeds) else 'dog'\n",
    "    \n",
    "    return {\n",
    "        'name': f\"Pet_{os.path.basename(filename).split('.')[0]}\",\n",
    "        'species': species,\n",
    "        'breed': breed,\n",
    "        'status': 'lost' if random.random() > 0.5 else 'found',\n",
    "        'location': random.choice(['Downtown', 'Uptown', 'Midtown', 'Suburb', 'Park']),\n",
    "        'date_added': (datetime.now() - pd.Timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d'),\n",
    "        'image_path': filename\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our feature extractor and database\n",
    "feature_extractor = PetFeatureExtractor(model_type=\"resnet50\")\n",
    "pet_db = PetDatabase()\n",
    "\n",
    "# Load a subset of images\n",
    "image_dir = os.path.join(data_dir, \"images\")\n",
    "image_paths = get_image_paths(image_dir, limit=200)\n",
    "\n",
    "# If we have images, process them\n",
    "if image_paths:\n",
    "    print(f\"Processing {len(image_paths)} images...\")\n",
    "    \n",
    "    # Extract features for all images\n",
    "    features = feature_extractor.extract_features_batch(image_paths)\n",
    "    \n",
    "    # Add each pet to the database\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        metadata = extract_metadata_from_filename(img_path)\n",
    "        pet_db.add_pet(metadata, features[i])\n",
    "    \n",
    "    print(f\"Added {len(image_paths)} pets to the database\")\n",
    "else:\n",
    "    print(\"No images found. Please download the dataset or provide sample images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Feature Space\n",
    "\n",
    "Let's visualize the feature space to better understand how our model separates different breeds and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_space(db, n_samples=100):\n",
    "    \"\"\"Visualize the feature space using t-SNE\"\"\"\n",
    "    if len(db.features) == 0:\n",
    "        print(\"No features to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Get features and metadata\n",
    "    features = db.get_features_array()\n",
    "    pets_df = db.get_all_pets()\n",
    "    \n",
    "    # If we have too many samples, subsample\n",
    "    if len(features) > n_samples:\n",
    "        indices = np.random.choice(len(features), n_samples, replace=False)\n",
    "        features = features[indices]\n",
    "        pets_df = pets_df.iloc[indices].reset_index(drop=True)\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    print(\"Applying t-SNE dimensionality reduction...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    # Create a DataFrame with t-SNE features\n",
    "    tsne_df = pd.DataFrame({\n",
    "        'x': features_2d[:, 0],\n",
    "        'y': features_2d[:, 1],\n",
    "        'breed': pets_df['breed'],\n",
    "        'species': pets_df['species'],\n",
    "        'status': pets_df['status']\n",
    "    })\n",
    "    \n",
    "    # Plot by species\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.scatterplot(data=tsne_df, x='x', y='y', hue='species', style='status', s=100, alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Pet Features by Species')\n",
    "    plt.legend(title='Species', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot by breed (only show top 10 breeds for clarity)\n",
    "    top_breeds = pets_df['breed'].value_counts().head(10).index.tolist()\n",
    "    breed_tsne_df = tsne_df[tsne_df['breed'].isin(top_breeds)]\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.scatterplot(data=breed_tsne_df, x='x', y='y', hue='breed', s=100, alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Pet Features by Breed (Top 10)')\n",
    "    plt.legend(title='Breed', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to visualize the feature space\n",
    "visualize_feature_space(pet_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulating a Lost Pet Search\n",
    "\n",
    "Now, let's simulate a search for a lost pet. We'll pick a random pet from our database, pretend it's a lost pet, and search for similar pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pet_search(db, feature_extractor, num_pets=5):\n",
    "    \"\"\"Simulate searching for lost pets\"\"\"\n",
    "    if len(db.pets_df) == 0:\n",
    "        print(\"No pets in the database to search for.\")\n",
    "        return\n",
    "    \n",
    "    # Select random pets to search for\n",
    "    sample_pets = db.pets_df.sample(num_pets)\n",
    "    \n",
    "    for _, pet in sample_pets.iterrows():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Searching for similar pets to: {pet['name']} ({pet['breed']}, {pet['species']})\")\n",
    "        print(f\"Status: {pet['status']}, Location: {pet['location']}\")\n",
    "        \n",
    "        # Display the pet image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        img = image.load_img(pet['image_path'])\n",
    "        plt.imshow(np.array(img))\n",
    "        plt.title(f\"Query: {pet['breed']}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Get the index of this pet in our features list\n",
    "        pet_idx = db.pets_df[db.pets_df['id'] == pet['id']].index[0]\n",
    "        query_features = db.features[pet_idx]\n",
    "        \n",
    "        # Search for similar pets\n",
    "        results = db.search_similar_pets(query_features, top_n=5)\n",
    "        \n",
    "        # Filter out the query pet itself\n",
    "        results = [r for r in results if r['id'] != pet['id']]\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No similar pets found.\")\n",
    "            continue\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nFound {len(results)} similar pets:\")\n",
    "        \n",
    "        plt.figure(figsize=(15, 4))\n",
    "        for i, result in enumerate(results[:4]):\n",
    "            plt.subplot(1, 4, i+1)\n",
    "            img = image.load_img(result['image_path'])\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.title(f\"Match {i+1}: {result['breed']}\\nScore: {result['similarity']:.2f}\")\n",
    "            plt.axis('off')\n",
    "            print(f\"{i+1}. {result['breed']} ({result['species']})\")\n",
    "            print(f\"   Similarity: {result['similarity']:.2f}\")\n",
    "            print(f\"   Status: {result['status']}, Location: {result['location']}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to simulate a pet search\n",
    "simulate_pet_search(pet_db, feature_extractor, num_pets=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improving Feature Extraction\n",
    "\n",
    "The basic ResNet50 features might not be specific enough for pet identification. Let's implement some additional techniques to improve our feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPetFeatureExtractor(PetFeatureExtractor):\n",
    "    def __init__(self, model_type=\"resnet50\", use_color_histogram=True, use_texture_features=True):\n",
    "        super().__init__(model_type)\n",
    "        self.use_color_histogram = use_color_histogram\n",
    "        self.use_texture_features = use_texture_features\n",
    "    \n",
    "    def compute_color_histogram(self, img_array):\n",
    "        \"\"\"Compute color histogram features with HSV color space\"\"\"\n",
    "        # Convert to HSV color space (better for color analysis)\n",
    "        hsv_img = rgb2hsv(img_array / 255.0)\n",
    "        \n",
    "        # Extract histograms for each channel\n",
    "        h_hist = np.histogram(hsv_img[:,:,0], bins=32, range=(0, 1))[0]\n",
    "        s_hist = np.histogram(hsv_img[:,:,1], bins=32, range=(0, 1))[0]\n",
    "        v_hist = np.histogram(hsv_img[:,:,2], bins=32, range=(0, 1))[0]\n",
    "        \n",
    "        # Normalize histograms\n",
    "        h_hist = h_hist / np.sum(h_hist) if np.sum(h_hist) > 0 else h_hist\n",
    "        s_hist = s_hist / np.sum(s_hist) if np.sum(s_hist) > 0 else s_hist\n",
    "        v_hist = v_hist / np.sum(v_hist) if np.sum(v_hist) > 0 else v_hist\n",
    "        \n",
    "        # Add statistical features (mean and std) for each channel\n",
    "        h_mean, h_std = np.mean(hsv_img[:,:,0]), np.std(hsv_img[:,:,0])\n",
    "        s_mean, s_std = np.mean(hsv_img[:,:,1]), np.std(hsv_img[:,:,1])\n",
    "        v_mean, v_std = np.mean(hsv_img[:,:,2]), np.std(hsv_img[:,:,2])\n",
    "        \n",
    "        # Combine features\n",
    "        color_features = np.concatenate([\n",
    "            h_hist, s_hist, v_hist,\n",
    "            [h_mean, h_std, s_mean, s_std, v_mean, v_std]\n",
    "        ])\n",
    "        \n",
    "        return color_features\n",
    "    \n",
    "    def compute_texture_features(self, img_array):\n",
    "        \"\"\"Compute texture features using Local Binary Patterns\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Parameters for LBP\n",
    "        radius = 3\n",
    "        n_points = 8 * radius\n",
    "        \n",
    "        # Compute LBP\n",
    "        lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "        \n",
    "        # Compute histogram of LBP\n",
    "        n_bins = n_points + 2  # uniform LBP has n_points + 2 distinct values\n",
    "        lbp_hist, _ = np.histogram(lbp, bins=n_bins, range=(0, n_bins))\n",
    "        \n",
    "        # Normalize histogram\n",
    "        lbp_hist = lbp_hist / np.sum(lbp_hist) if np.sum(lbp_hist) > 0 else lbp_hist\n",
    "        \n",
    "        return lbp_hist\n",
    "    \n",
    "    def extract_features(self, img_path):\n",
    "        \"\"\"Extract enhanced features from a single image\"\"\"\n",
    "        # Load and preprocess the image\n",
    "        img = image.load_img(img_path, target_size=(self.input_shape[0], self.input_shape[1]))\n",
    "        img_array = image.img_to_array(img)\n",
    "        \n",
    "        # Extract deep features\n",
    "        deep_features = super().extract_features(img_path)\n",
    "        \n",
    "        features_list = [deep_features]\n",
    "        \n",
    "        # Add color histogram features\n",
    "        if self.use_color_histogram:\n",
    "            color_features = self.compute_color_histogram(img_array)\n",
    "            features_list.append(color_features)\n",
    "        \n",
    "        # Add texture features\n",
    "        if self.use_texture_features:\n",
    "            texture_features = self.compute_texture_features(img_array)\n",
    "            features_list.append(texture_features)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = np.concatenate(features_list)\n",
    "        return combined_features\n",
    "    \n",
    "    def extract_features_batch(self, img_paths, batch_size=16):\n",
    "        \"\"\"Extract enhanced features from a batch of images\"\"\"\n",
    "        # Process in smaller batches due to additional feature extraction\n",
    "        features = []\n",
    "        for img_path in tqdm(img_paths):\n",
    "            try:\n",
    "                features.append(self.extract_features(img_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                # Use zeros with the right dimension\n",
    "                if len(features) > 0:\n",
    "                    features.append(np.zeros_like(features[0]))\n",
    "                else:\n",
    "                    # Make an educated guess about the feature size\n",
    "                    deep_feature_size = 2048  # for ResNet50\n",
    "                    color_hist_size = 102  # 32 bins * 3 channels + 6 statistics\n",
    "                    texture_hist_size = 26  # uniform LBP with radius 3\n",
    "                    total_size = deep_feature_size\n",
    "                    if self.use_color_histogram:\n",
    "                        total_size += color_hist_size\n",
    "                    if self.use_texture_features:\n",
    "                        total_size += texture_hist_size\n",
    "                    features.append(np.zeros(total_size))\n",
    "        \n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demo with Enhanced Features\n",
    "\n",
    "Let's try our enhanced feature extractor and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our enhanced feature extractor and a new database\n",
    "enhanced_extractor = EnhancedPetFeatureExtractor(model_type=\"resnet50\")\n",
    "enhanced_db = PetDatabase()\n",
    "\n",
    "# If we have images, process them with enhanced features\n",
    "if image_paths:\n",
    "    print(f\"Processing {len(image_paths)} images with enhanced features...\")\n",
    "    \n",
    "    # Extract enhanced features for all images\n",
    "    enhanced_features = enhanced_extractor.extract_features_batch(image_paths)\n",
    "    \n",
    "    # Add each pet to the database with enhanced features\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        metadata = extract_metadata_from_filename(img_path)\n",
    "        enhanced_db.add_pet(metadata, enhanced_features[i])\n",
    "    \n",
    "    print(f\"Added {len(image_paths)} pets to the enhanced database\")\n",
    "    \n",
    "    # Simulate a pet search with enhanced features\n",
    "    # Uncomment to run the simulation\n",
    "    simulate_pet_search(enhanced_db, enhanced_extractor, num_pets=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adding Metadata to Improve Matching\n",
    "\n",
    "Now, let's incorporate metadata like location to improve our search results. We'll implement a weighted search that combines feature similarity with metadata matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataEnhancedPetDatabase(PetDatabase):\n",
    "    def search_similar_pets(self, query_features, query_metadata=None, top_n=5, feature_weight=0.7):\n",
    "        \"\"\"Find pets with similar features and metadata\"\"\"\n",
    "        if len(self.features) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Calculate feature similarity\n",
    "        features_array = np.array(self.features)\n",
    "        feature_similarities = cosine_similarity([query_features], features_array)[0]\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = self.pets_df.copy()\n",
    "        results_df['feature_similarity'] = feature_similarities\n",
    "        \n",
    "        # Calculate metadata similarity if provided\n",
    "        if query_metadata:\n",
    "            # Initialize metadata similarity score\n",
    "            results_df['metadata_similarity'] = 0.0\n",
    "            \n",
    "            # Check location match\n",
    "            if 'location' in query_metadata and query_metadata['location']:\n",
    "                results_df.loc[results_df['location'] == query_metadata['location'], 'metadata_similarity'] += 0.5\n",
    "            \n",
    "            # Check species match\n",
    "            if 'species' in query_metadata and query_metadata['species']:\n",
    "                results_df.loc[results_df['species'] == query_metadata['species'], 'metadata_similarity'] += 0.3\n",
    "            \n",
    "            # Check breed match\n",
    "            if 'breed' in query_metadata and query_metadata['breed']:\n",
    "                results_df.loc[results_df['breed'] == query_metadata['breed'], 'metadata_similarity'] += 0.2\n",
    "            \n",
    "            # Combine feature and metadata similarity\n",
    "            results_df['similarity'] = (\n",
    "                feature_weight * results_df['feature_similarity'] + \n",
    "                (1 - feature_weight) * results_df['metadata_similarity']\n",
    "            )\n",
    "        else:\n",
    "            # Use only feature similarity\n",
    "            results_df['similarity'] = results_df['feature_similarity']\n",
    "        \n",
    "        # Sort by combined similarity and get top N\n",
    "        results_df = results_df.sort_values('similarity', ascending=False).head(top_n)\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        results = results_df.to_dict('records')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our metadata-enhanced database\n",
    "metadata_db = MetadataEnhancedPetDatabase()\n",
    "\n",
    "# If we have images, process them\n",
    "if image_paths:\n",
    "    print(f\"Processing {len(image_paths)} images for the metadata-enhanced database...\")\n",
    "    \n",
    "    # Reuse the enhanced features\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        metadata = extract_metadata_from_filename(img_path)\n",
    "        metadata_db.add_pet(metadata, enhanced_features[i])\n",
    "    \n",
    "    print(f\"Added {len(image_paths)} pets to the metadata-enhanced database\")\n",
    "    \n",
    "    # Define a function to simulate search with metadata\n",
    "    def simulate_metadata_search(db, extractor, num_pets=3):\n",
    "        \"\"\"Simulate search with metadata enhancement\"\"\"\n",
    "        if len(db.pets_df) == 0:\n",
    "            print(\"No pets in the database to search for.\")\n",
    "            return\n",
    "        \n",
    "        # Select random lost pets to search for\n",
    "        lost_pets = db.pets_df[db.pets_df['status'] == 'lost'].sample(num_pets)\n",
    "        \n",
    "        for _, pet in lost_pets.iterrows():\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Searching for similar pets to: {pet['name']} ({pet['breed']}, {pet['species']})\")\n",
    "            print(f\"Location: {pet['location']}\")\n",
    "            \n",
    "            # Display the pet image\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            img = image.load_img(pet['image_path'])\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.title(f\"Lost pet: {pet['breed']}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Get the index of this pet in our features list\n",
    "            pet_idx = db.pets_df[db.pets_df['id'] == pet['id']].index[0]\n",
    "            query_features = db.features[pet_idx]\n",
    "            \n",
    "            # Create query metadata\n",
    "            query_metadata = {\n",
    "                'species': pet['species'],\n",
    "                'location': pet['location']\n",
    "            }\n",
    "            \n",
    "            # Search with feature similarity only\n",
    "            feature_results = db.search_similar_pets(query_features, top_n=3)\n",
    "            # Filter out the query pet itself\n",
    "            feature_results = [r for r in feature_results if r['id'] != pet['id']]\n",
    "            \n",
    "            # Search with metadata enhancement\n",
    "            metadata_results = db.search_similar_pets(query_features, query_metadata, top_n=3)\n",
    "            # Filter out the query pet itself\n",
    "            metadata_results = [r for r in metadata_results if r['id'] != pet['id']]\n",
    "            \n",
    "            # Display feature-only results\n",
    "            if feature_results:\n",
    "                print(\"\\nResults using only image features:\")\n",
    "                plt.figure(figsize=(15, 4))\n",
    "                for i, result in enumerate(feature_results[:3]):\n",
    "                    plt.subplot(1, 3, i+1)\n",
    "                    img = image.load_img(result['image_path'])\n",
    "                    plt.imshow(np.array(img))\n",
    "                    plt.title(f\"Match {i+1}: {result['breed']}\\nScore: {result['similarity']:.2f}\")\n",
    "                    plt.axis('off')\n",
    "                    print(f\"{i+1}. {result['breed']} ({result['species']})\")\n",
    "                    print(f\"   Similarity: {result['similarity']:.2f}\")\n",
    "                    print(f\"   Location: {result['location']}\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Display metadata-enhanced results\n",
    "            if metadata_results:\n",
    "                print(\"\\nResults using image features + metadata:\")\n",
    "                plt.figure(figsize=(15, 4))\n",
    "                for i, result in enumerate(metadata_results[:3]):\n",
    "                    plt.subplot(1, 3, i+1)\n",
    "                    img = image.load_img(result['image_path'])\n",
    "                    plt.imshow(np.array(img))\n",
    "                    plt.title(f\"Match {i+1}: {result['breed']}\\nScore: {result['similarity']:.2f}\")\n",
    "                    plt.axis('off')\n",
    "                    print(f\"{i+1}. {result['breed']} ({result['species']})\")\n",
    "                    print(f\"   Similarity: {result['similarity']:.2f}\")\n",
    "                    print(f\"   Location: {result['location']}\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    # Uncomment to run the metadata-enhanced search\n",
    "    simulate_metadata_search(metadata_db, enhanced_extractor, num_pets=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
